{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Forward & Backward Pass in Neural Networks — with NumPy\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "* Manual implementation of a 2-layer NN\n",
        "\n",
        "* How gradients flow backward to update weights\n",
        "\n",
        "* How ReLU and Sigmoid affect learning\n",
        "\n",
        "* Training using gradient descent"
      ],
      "metadata": {
        "id": "zySiUbt2TwZ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhCd24i7S5CL",
        "outputId": "06ebab2b-4aca-4cd8-a430-408449d71aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.6107\n",
            "Epoch 100 - Loss: 0.1022\n",
            "Epoch 200 - Loss: 0.0440\n",
            "Epoch 300 - Loss: 0.0272\n",
            "Epoch 400 - Loss: 0.0193\n",
            "Epoch 500 - Loss: 0.0149\n",
            "Epoch 600 - Loss: 0.0121\n",
            "Epoch 700 - Loss: 0.0102\n",
            "Epoch 800 - Loss: 0.0088\n",
            "Epoch 900 - Loss: 0.0077\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Input (2 samples, 2 features)\n",
        "X = np.array([[0.5, 0.1],\n",
        "              [0.9, 0.8]])\n",
        "\n",
        "# True labels (binary classification)\n",
        "Y = np.array([[1],\n",
        "              [0]])\n",
        "\n",
        "# Initialize weights and biases\n",
        "W1 = np.random.randn(2, 2)    # input to hidden (2x2)\n",
        "b1 = np.zeros((1, 2))         # hidden layer bias\n",
        "\n",
        "W2 = np.random.randn(2, 1)    # hidden to output (2x1)\n",
        "b2 = np.zeros((1, 1))         # output layer bias\n",
        "\n",
        "# Activation functions\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(a):\n",
        "    return a * (1 - a)\n",
        "\n",
        "# Training hyperparameters\n",
        "learning_rate = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # ----------- FORWARD PASS ------------\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = sigmoid(Z2)  # prediction\n",
        "\n",
        "    # ----------- LOSS (Binary Cross-Entropy) ------------\n",
        "    loss = -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
        "\n",
        "    # ----------- BACKWARD PASS ------------\n",
        "    dZ2 = A2 - Y                            # error at output\n",
        "    dW2 = np.dot(A1.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    # ----------- UPDATE WEIGHTS ------------\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    # ----------- OPTIONAL: print loss ------------\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch} - Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ll see the loss value decrease every 100 epochs, showing that the network is learning"
      ],
      "metadata": {
        "id": "R9dVFWYUTsGG"
      }
    }
  ]
}